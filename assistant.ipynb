{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reshape position embedding from 196 to 576\n",
      "load checkpoint from /GPFS/rhome/kejinli/workspace/multi-modal/BLIP/BLIP4caption/pretrained_model/model_base_capfilt_large.pth\n",
      "Using downloaded and verified file: /GPFS/rhome/kejinli/workspace/multi-modal/BLIP/BLIP4caption/caption_data/coco/annotations/coco_karpathy_train.json\n",
      "Using downloaded and verified file: /GPFS/rhome/kejinli/workspace/multi-modal/BLIP/BLIP4caption/caption_data/coco/annotations/coco_karpathy_val.json\n",
      "Using downloaded and verified file: /GPFS/rhome/kejinli/workspace/multi-modal/BLIP/BLIP4caption/caption_data/coco/annotations/coco_karpathy_test.json\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "import argparse\n",
    "import os\n",
    "os.environ[\"HUGGINGFACE_HUB_URL\"] = \"https://hf-mirror.com\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4,5,6,7\"\n",
    "\n",
    "from ruamel.yaml import YAML\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from models.blip import blip_decoder\n",
    "import utils\n",
    "\n",
    "from utils import cosine_lr_schedule\n",
    "\n",
    "from data import create_dataset, create_sampler, create_loader\n",
    "\n",
    "from data.utils import save_result, coco_caption_eval\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--config', default='/GPFS/rhome/kejinli/workspace/multi-modal/BLIP/BLIP4caption/configs/caption_coco.yaml')\n",
    "parser.add_argument('--output_dir', default='output/Caption_coco')        \n",
    "parser.add_argument('--evaluate', action='store_true')    \n",
    "parser.add_argument('--device', default='cuda')\n",
    "parser.add_argument('--seed', default=42, type=int)\n",
    "parser.add_argument('--world_size', default=1, type=int, help='number of distributed processes')    \n",
    "parser.add_argument('--dist_url', default='env://', help='url used to set up distributed training')\n",
    "parser.add_argument('--distributed', default=True, type=bool)\n",
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "yaml = YAML(typ='rt')\n",
    "with open(args.config, 'r') as f:\n",
    "    config = yaml.load(f)\n",
    "\n",
    "args.result_dir = os.path.join(args.output_dir, 'result')\n",
    "\n",
    "Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "Path(args.result_dir).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "yaml = YAML()\n",
    "with open(os.path.join(args.output_dir, 'config.yaml'), 'w') as f:\n",
    "    yaml.dump(config, f)\n",
    "\n",
    "# model = blip_decoder(pretrained=config['pretrained'], image_size=config['image_size'], vit=config['vit'], \n",
    "#                            vit_grad_ckpt=config['vit_grad_ckpt'], vit_ckpt_layer=config['vit_ckpt_layer'], \n",
    "#                            prompt=config['prompt'])\n",
    "\n",
    "# from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "# train_dataset, val_dataset, test_dataset = create_dataset('caption_coco', config)  \n",
    "# train_subset = Subset(train_dataset, range(12))\n",
    "# val_subset = Subset(val_dataset, range(120))\n",
    "# test_subset = Subset(test_dataset, range(120))\n",
    "\n",
    "# if args.distributed:\n",
    "#     num_tasks = utils.get_world_size()\n",
    "#     global_rank = utils.get_rank()            \n",
    "#     samplers = create_sampler([train_subset,val_subset,test_subset], [True,False,False], num_tasks, global_rank)         \n",
    "# else:\n",
    "#     samplers = [None, None, None]\n",
    "    \n",
    "# train_loader, val_loader, test_loader = create_loader([train_subset, val_subset, test_subset],samplers,\n",
    "#                                                           batch_size=[config['batch_size']]*3,num_workers=[4,4,4],\n",
    "#                                                           is_trains=[True, False, False], collate_fns=[None,None,None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 19])\n"
     ]
    }
   ],
   "source": [
    "for a in train_loader:\n",
    "    #print(len(a))\n",
    "    pass\n",
    "\n",
    "image = a[0]\n",
    "caption = a[1]\n",
    "\n",
    "loss = model(image, caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lora Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using distributed mode\n",
      "*******************************************************************************************************\n",
      "Log:\n",
      "Device: cuda\n",
      "Creating captioning dataset\n",
      "Using downloaded and verified file: /GPFS/rhome/kejinli/workspace/multi-modal/BLIP/BLIP4caption/caption_data/coco/annotations/coco_karpathy_train.json\n",
      "Using downloaded and verified file: /GPFS/rhome/kejinli/workspace/multi-modal/BLIP/BLIP4caption/caption_data/coco/annotations/coco_karpathy_val.json\n",
      "Using downloaded and verified file: /GPFS/rhome/kejinli/workspace/multi-modal/BLIP/BLIP4caption/caption_data/coco/annotations/coco_karpathy_test.json\n",
      "Creating model\n",
      "reshape position embedding from 196 to 576\n",
      "load checkpoint from /GPFS/rhome/kejinli/workspace/multi-modal/BLIP/BLIP4caption/pretrained_model/model_base_capfilt_large.pth\n",
      "Model is loaded\n",
      "LoRA is loaded\n",
      "*******************************************************************************************************\n",
      "5000\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "import argparse\n",
    "import os\n",
    "os.environ[\"HUGGINGFACE_HUB_URL\"] = \"https://hf-mirror.com\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4,5,6,7\"\n",
    "\n",
    "from ruamel.yaml import YAML\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from models.blip import blip_decoder\n",
    "import utils\n",
    "\n",
    "from utils import cosine_lr_schedule\n",
    "\n",
    "from data import create_dataset, create_sampler, create_loader\n",
    "\n",
    "from data.utils import save_result, coco_caption_eval\n",
    "from peft import get_peft_model, LoraConfig\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--config', default='/GPFS/rhome/kejinli/workspace/multi-modal/BLIP/BLIP4caption/configs/caption_coco.yaml')\n",
    "parser.add_argument('--output_dir', default='output/Caption_coco')        \n",
    "parser.add_argument('--evaluate', action='store_true')    \n",
    "parser.add_argument('--device', default='cuda')\n",
    "parser.add_argument('--seed', default=42, type=int)\n",
    "parser.add_argument('--world_size', default=1, type=int, help='number of distributed processes')    \n",
    "parser.add_argument('--dist_url', default='env://', help='url used to set up distributed training')\n",
    "parser.add_argument('--distributed', default=True, type=bool)\n",
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "yaml = YAML(typ='rt')\n",
    "with open(args.config, 'r') as f:\n",
    "    config = yaml.load(f)\n",
    "\n",
    "args.result_dir = os.path.join(args.output_dir, 'result')\n",
    "\n",
    "Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "Path(args.result_dir).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "yaml = YAML()\n",
    "with open(os.path.join(args.output_dir, 'config.yaml'), 'w') as f:\n",
    "    yaml.dump(config, f)\n",
    "\n",
    "utils.init_distributed_mode(args)\n",
    "    \n",
    "device = torch.device(args.device)\n",
    "print('*******************************************************************************************************')\n",
    "print('Log:')\n",
    "print('Device:', device) \n",
    "    \n",
    "# fix the seed for reproducibility\n",
    "seed = args.seed + utils.get_rank()\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "cudnn.benchmark = True\n",
    "\n",
    "#### Dataset #### \n",
    "print(\"Creating captioning dataset\")\n",
    "\n",
    "#*******************************************************************************************************\n",
    "train_dataset, val_dataset, test_dataset = create_dataset('caption_coco', config)  \n",
    "\n",
    "if args.distributed:\n",
    "    num_tasks = utils.get_world_size()\n",
    "    global_rank = utils.get_rank()            \n",
    "    samplers = create_sampler([train_dataset,val_dataset,test_dataset], [True,False,False], num_tasks, global_rank)         \n",
    "else:\n",
    "    samplers = [None, None, None]\n",
    "\n",
    "train_loader, val_loader, test_loader = create_loader([train_dataset, val_dataset, test_dataset],samplers,\n",
    "                                                        batch_size=[config['batch_size']]*3,num_workers=[4,4,4],\n",
    "                                                        is_trains=[True, False, False], collate_fns=[None,None,None])\n",
    "#*******************************************************************************************************\n",
    "# from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "# train_dataset, val_dataset, test_dataset = create_dataset('caption_coco', config)  \n",
    "# train_subset = Subset(train_dataset, range(12000))\n",
    "# val_subset = Subset(val_dataset, range(2400))\n",
    "# test_subset = Subset(test_dataset, range(2400))\n",
    "\n",
    "# if args.distributed:\n",
    "#     num_tasks = utils.get_world_size()\n",
    "#     global_rank = utils.get_rank()            \n",
    "#     samplers = create_sampler([train_subset,val_subset,test_subset], [True,False,False], num_tasks, global_rank)         \n",
    "# else:\n",
    "#     samplers = [None, None, None]\n",
    "\n",
    "# train_loader, val_loader, test_loader = create_loader([train_subset, val_subset, test_subset],samplers,\n",
    "#                                                       batch_size=[config['batch_size']]*3,num_workers=[4,4,4],\n",
    "#                                                       is_trains=[True, False, False], collate_fns=[None,None,None])      \n",
    "#*******************************************************************************************************\n",
    "\n",
    "\n",
    "#### Model ####\n",
    "print(\"Creating model\")\n",
    "model = blip_decoder(pretrained=config['pretrained'], image_size=config['image_size'], vit=config['vit'], \n",
    "                        vit_grad_ckpt=config['vit_grad_ckpt'], vit_ckpt_layer=config['vit_ckpt_layer'], \n",
    "                        prompt=config['prompt'])\n",
    "\n",
    "#model = model.to(device)\n",
    "print('Model is loaded')   \n",
    "\n",
    "target_modules_image = []\n",
    "target_modules_text = []\n",
    "for i in range(12):  # depth=12\n",
    "    target_modules_image.extend([\n",
    "        f\"blocks.{i}.attn.qkv\",\n",
    "        f\"blocks.{i}.attn.proj\",\n",
    "        f\"blocks.{i}.mlp.fc1\",\n",
    "        f\"blocks.{i}.mlp.fc2\"\n",
    "    ])\n",
    "\n",
    "for i in range(12):\n",
    "    target_modules_text.extend([\n",
    "        f\"bert.encoder.layer.{i}.attention.self.query\",\n",
    "        f\"bert.encoder.layer.{i}.attention.self.key\",\n",
    "        f\"bert.encoder.layer.{i}.attention.self.value\",\n",
    "        f\"bert.encoder.layer.{i}.attention.output.dense\",\n",
    "        f\"bert.encoder.layer.{i}.intermediate.dense\",\n",
    "        f\"bert.encoder.layer.{i}.output.dense\",\n",
    "        f\"bert.encoder.layer.{i}.crossattention.self.query\",\n",
    "        f\"bert.encoder.layer.{i}.crossattention.self.key\",\n",
    "        f\"bert.encoder.layer.{i}.crossattention.self.value\",\n",
    "        f\"bert.encoder.layer.{i}.crossattention.output.dense\",\n",
    "    ])\n",
    "\n",
    "lora_config_image= LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=32,\n",
    "        target_modules=target_modules_image,\n",
    "        lora_dropout=0.1\n",
    "    )\n",
    "\n",
    "lora_config_text= LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=32,\n",
    "        target_modules=target_modules_text,\n",
    "        lora_dropout=0.1\n",
    "    )\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "from transformers import PretrainedConfig\n",
    "\n",
    "visual_encoder_config = PretrainedConfig(\n",
    "    img_size=384, \n",
    "    patch_size=16, \n",
    "    in_chans=3, \n",
    "    num_classes=1000, \n",
    "    embed_dim=768, \n",
    "    depth=12,\n",
    "    num_heads=12, \n",
    "    mlp_ratio=4., \n",
    "    qkv_bias=True, \n",
    "    qk_scale=None, \n",
    "    representation_size=None,\n",
    "    drop_rate=0., \n",
    "    attn_drop_rate=0., \n",
    "    drop_path_rate=0., \n",
    "    norm_layer=None, \n",
    "    use_grad_checkpointing=False, \n",
    "    ckpt_layer=0\n",
    ")\n",
    "\n",
    "model.visual_encoder.config = visual_encoder_config\n",
    "\n",
    "model.visual_encoder = get_peft_model(model.visual_encoder, lora_config_image)\n",
    "model.text_decoder = get_peft_model(model.text_decoder, lora_config_text)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "print('LoRA is loaded')\n",
    "print('*******************************************************************************************************')\n",
    "print(len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visual_encoder.base_model.model.blocks.0.attn.qkv.lora_A.weight\n",
      "visual_encoder.base_model.model.blocks.0.attn.qkv.lora_B.weight\n",
      "visual_encoder.base_model.model.blocks.0.attn.proj.lora_A.weight\n",
      "visual_encoder.base_model.model.blocks.0.attn.proj.lora_B.weight\n",
      "visual_encoder.base_model.model.blocks.0.mlp.fc1.lora_A.weight\n",
      "visual_encoder.base_model.model.blocks.0.mlp.fc1.lora_B.weight\n",
      "visual_encoder.base_model.model.blocks.0.mlp.fc2.lora_A.weight\n",
      "visual_encoder.base_model.model.blocks.0.mlp.fc2.lora_B.weight\n",
      "visual_encoder.base_model.model.blocks.1.attn.qkv.lora_A.weight\n",
      "visual_encoder.base_model.model.blocks.1.attn.qkv.lora_B.weight\n",
      "visual_encoder.base_model.model.blocks.1.attn.proj.lora_A.weight\n",
      "visual_encoder.base_model.model.blocks.1.attn.proj.lora_B.weight\n",
      "visual_encoder.base_model.model.blocks.1.mlp.fc1.lora_A.weight\n",
      "visual_encoder.base_model.model.blocks.1.mlp.fc1.lora_B.weight\n",
      "visual_encoder.base_model.model.blocks.1.mlp.fc2.lora_A.weight\n",
      "visual_encoder.base_model.model.blocks.1.mlp.fc2.lora_B.weight\n",
      "visual_encoder.base_model.model.blocks.2.attn.qkv.lora_A.weight\n",
      "visual_encoder.base_model.model.blocks.2.attn.qkv.lora_B.weight\n",
      "visual_encoder.base_model.model.blocks.2.attn.proj.lora_A.weight\n",
      "visual_encoder.base_model.model.blocks.2.attn.proj.lora_B.weight\n",
      "visual_encoder.base_model.model.blocks.2.mlp.fc1.lora_A.weight\n",
      "visual_encoder.base_model.model.blocks.2.mlp.fc1.lora_B.weight\n",
      "visual_encoder.base_model.model.blocks.2.mlp.fc2.lora_A.weight\n",
      "visual_encoder.base_model.model.blocks.2.mlp.fc2.lora_B.weight\n",
      "visual_encoder.base_model.model.blocks.3.attn.qkv.lora_A.weight\n",
      "visual_encoder.base_model.model.blocks.3.attn.qkv.lora_B.weight\n",
      "visual_encoder.base_model.model.blocks.3.attn.proj.lora_A.weight\n",
      "visual_encoder.base_model.model.blocks.3.attn.proj.lora_B.weight\n",
      "visual_encoder.base_model.model.blocks.3.mlp.fc1.lora_A.weight\n",
      "visual_encoder.base_model.model.blocks.3.mlp.fc1.lora_B.weight\n",
      "visual_encoder.base_model.model.blocks.3.mlp.fc2.lora_A.weight\n",
      "visual_encoder.base_model.model.blocks.3.mlp.fc2.lora_B.weight\n",
      "visual_encoder.base_model.model.blocks.4.attn.qkv.lora_A.weight\n",
      "visual_encoder.base_model.model.blocks.4.attn.qkv.lora_B.weight\n",
      "visual_encoder.base_model.model.blocks.4.attn.proj.lora_A.weight\n",
      "visual_encoder.base_model.model.blocks.4.attn.proj.lora_B.weight\n",
      "visual_encoder.base_model.model.blocks.4.mlp.fc1.lora_A.weight\n",
      "visual_encoder.base_model.model.blocks.4.mlp.fc1.lora_B.weight\n",
      "visual_encoder.base_model.model.blocks.4.mlp.fc2.lora_A.weight\n",
      "visual_encoder.base_model.model.blocks.4.mlp.fc2.lora_B.weight\n",
      "visual_encoder.base_model.model.blocks.5.attn.qkv.lora_A.weight\n",
      "visual_encoder.base_model.model.blocks.5.attn.qkv.lora_B.weight\n",
      "visual_encoder.base_model.model.blocks.5.attn.proj.lora_A.weight\n",
      "visual_encoder.base_model.model.blocks.5.attn.proj.lora_B.weight\n",
      "visual_encoder.base_model.model.blocks.5.mlp.fc1.lora_A.weight\n",
      "visual_encoder.base_model.model.blocks.5.mlp.fc1.lora_B.weight\n",
      "visual_encoder.base_model.model.blocks.5.mlp.fc2.lora_A.weight\n",
      "visual_encoder.base_model.model.blocks.5.mlp.fc2.lora_B.weight\n",
      "visual_encoder.base_model.model.blocks.6.attn.qkv.lora_A.weight\n",
      "visual_encoder.base_model.model.blocks.6.attn.qkv.lora_B.weight\n",
      "visual_encoder.base_model.model.blocks.6.attn.proj.lora_A.weight\n",
      "visual_encoder.base_model.model.blocks.6.attn.proj.lora_B.weight\n",
      "visual_encoder.base_model.model.blocks.6.mlp.fc1.lora_A.weight\n",
      "visual_encoder.base_model.model.blocks.6.mlp.fc1.lora_B.weight\n",
      "visual_encoder.base_model.model.blocks.6.mlp.fc2.lora_A.weight\n",
      "visual_encoder.base_model.model.blocks.6.mlp.fc2.lora_B.weight\n",
      "visual_encoder.base_model.model.blocks.7.attn.qkv.lora_A.weight\n",
      "visual_encoder.base_model.model.blocks.7.attn.qkv.lora_B.weight\n",
      "visual_encoder.base_model.model.blocks.7.attn.proj.lora_A.weight\n",
      "visual_encoder.base_model.model.blocks.7.attn.proj.lora_B.weight\n",
      "visual_encoder.base_model.model.blocks.7.mlp.fc1.lora_A.weight\n",
      "visual_encoder.base_model.model.blocks.7.mlp.fc1.lora_B.weight\n",
      "visual_encoder.base_model.model.blocks.7.mlp.fc2.lora_A.weight\n",
      "visual_encoder.base_model.model.blocks.7.mlp.fc2.lora_B.weight\n",
      "visual_encoder.base_model.model.blocks.8.attn.qkv.lora_A.weight\n",
      "visual_encoder.base_model.model.blocks.8.attn.qkv.lora_B.weight\n",
      "visual_encoder.base_model.model.blocks.8.attn.proj.lora_A.weight\n",
      "visual_encoder.base_model.model.blocks.8.attn.proj.lora_B.weight\n",
      "visual_encoder.base_model.model.blocks.8.mlp.fc1.lora_A.weight\n",
      "visual_encoder.base_model.model.blocks.8.mlp.fc1.lora_B.weight\n",
      "visual_encoder.base_model.model.blocks.8.mlp.fc2.lora_A.weight\n",
      "visual_encoder.base_model.model.blocks.8.mlp.fc2.lora_B.weight\n",
      "visual_encoder.base_model.model.blocks.9.attn.qkv.lora_A.weight\n",
      "visual_encoder.base_model.model.blocks.9.attn.qkv.lora_B.weight\n",
      "visual_encoder.base_model.model.blocks.9.attn.proj.lora_A.weight\n",
      "visual_encoder.base_model.model.blocks.9.attn.proj.lora_B.weight\n",
      "visual_encoder.base_model.model.blocks.9.mlp.fc1.lora_A.weight\n",
      "visual_encoder.base_model.model.blocks.9.mlp.fc1.lora_B.weight\n",
      "visual_encoder.base_model.model.blocks.9.mlp.fc2.lora_A.weight\n",
      "visual_encoder.base_model.model.blocks.9.mlp.fc2.lora_B.weight\n",
      "visual_encoder.base_model.model.blocks.10.attn.qkv.lora_A.weight\n",
      "visual_encoder.base_model.model.blocks.10.attn.qkv.lora_B.weight\n",
      "visual_encoder.base_model.model.blocks.10.attn.proj.lora_A.weight\n",
      "visual_encoder.base_model.model.blocks.10.attn.proj.lora_B.weight\n",
      "visual_encoder.base_model.model.blocks.10.mlp.fc1.lora_A.weight\n",
      "visual_encoder.base_model.model.blocks.10.mlp.fc1.lora_B.weight\n",
      "visual_encoder.base_model.model.blocks.10.mlp.fc2.lora_A.weight\n",
      "visual_encoder.base_model.model.blocks.10.mlp.fc2.lora_B.weight\n",
      "visual_encoder.base_model.model.blocks.11.attn.qkv.lora_A.weight\n",
      "visual_encoder.base_model.model.blocks.11.attn.qkv.lora_B.weight\n",
      "visual_encoder.base_model.model.blocks.11.attn.proj.lora_A.weight\n",
      "visual_encoder.base_model.model.blocks.11.attn.proj.lora_B.weight\n",
      "visual_encoder.base_model.model.blocks.11.mlp.fc1.lora_A.weight\n",
      "visual_encoder.base_model.model.blocks.11.mlp.fc1.lora_B.weight\n",
      "visual_encoder.base_model.model.blocks.11.mlp.fc2.lora_A.weight\n",
      "visual_encoder.base_model.model.blocks.11.mlp.fc2.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.0.attention.self.query.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.0.attention.self.query.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.0.attention.self.key.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.0.attention.self.key.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.0.attention.self.value.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.0.attention.self.value.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.0.attention.output.dense.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.0.attention.output.dense.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.0.crossattention.self.query.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.0.crossattention.self.query.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.0.crossattention.self.key.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.0.crossattention.self.key.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.0.crossattention.self.value.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.0.crossattention.self.value.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.0.crossattention.output.dense.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.0.crossattention.output.dense.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.0.intermediate.dense.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.0.intermediate.dense.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.0.output.dense.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.0.output.dense.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.1.attention.self.query.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.1.attention.self.query.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.1.attention.self.key.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.1.attention.self.key.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.1.attention.self.value.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.1.attention.self.value.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.1.attention.output.dense.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.1.attention.output.dense.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.1.crossattention.self.query.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.1.crossattention.self.query.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.1.crossattention.self.key.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.1.crossattention.self.key.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.1.crossattention.self.value.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.1.crossattention.self.value.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.1.crossattention.output.dense.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.1.crossattention.output.dense.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.1.intermediate.dense.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.1.intermediate.dense.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.1.output.dense.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.1.output.dense.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.2.attention.self.query.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.2.attention.self.query.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.2.attention.self.key.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.2.attention.self.key.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.2.attention.self.value.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.2.attention.self.value.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.2.attention.output.dense.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.2.attention.output.dense.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.2.crossattention.self.query.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.2.crossattention.self.query.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.2.crossattention.self.key.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.2.crossattention.self.key.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.2.crossattention.self.value.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.2.crossattention.self.value.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.2.crossattention.output.dense.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.2.crossattention.output.dense.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.2.intermediate.dense.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.2.intermediate.dense.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.2.output.dense.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.2.output.dense.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.3.attention.self.query.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.3.attention.self.query.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.3.attention.self.key.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.3.attention.self.key.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.3.attention.self.value.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.3.attention.self.value.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.3.attention.output.dense.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.3.attention.output.dense.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.3.crossattention.self.query.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.3.crossattention.self.query.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.3.crossattention.self.key.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.3.crossattention.self.key.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.3.crossattention.self.value.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.3.crossattention.self.value.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.3.crossattention.output.dense.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.3.crossattention.output.dense.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.3.intermediate.dense.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.3.intermediate.dense.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.3.output.dense.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.3.output.dense.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.4.attention.self.query.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.4.attention.self.query.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.4.attention.self.key.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.4.attention.self.key.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.4.attention.self.value.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.4.attention.self.value.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.4.attention.output.dense.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.4.attention.output.dense.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.4.crossattention.self.query.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.4.crossattention.self.query.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.4.crossattention.self.key.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.4.crossattention.self.key.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.4.crossattention.self.value.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.4.crossattention.self.value.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.4.crossattention.output.dense.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.4.crossattention.output.dense.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.4.intermediate.dense.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.4.intermediate.dense.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.4.output.dense.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.4.output.dense.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.5.attention.self.query.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.5.attention.self.query.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.5.attention.self.key.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.5.attention.self.key.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.5.attention.self.value.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.5.attention.self.value.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.5.attention.output.dense.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.5.attention.output.dense.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.5.crossattention.self.query.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.5.crossattention.self.query.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.5.crossattention.self.key.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.5.crossattention.self.key.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.5.crossattention.self.value.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.5.crossattention.self.value.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.5.crossattention.output.dense.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.5.crossattention.output.dense.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.5.intermediate.dense.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.5.intermediate.dense.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.5.output.dense.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.5.output.dense.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.6.attention.self.query.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.6.attention.self.query.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.6.attention.self.key.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.6.attention.self.key.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.6.attention.self.value.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.6.attention.self.value.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.6.attention.output.dense.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.6.attention.output.dense.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.6.crossattention.self.query.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.6.crossattention.self.query.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.6.crossattention.self.key.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.6.crossattention.self.key.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.6.crossattention.self.value.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.6.crossattention.self.value.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.6.crossattention.output.dense.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.6.crossattention.output.dense.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.6.intermediate.dense.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.6.intermediate.dense.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.6.output.dense.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.6.output.dense.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.7.attention.self.query.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.7.attention.self.query.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.7.attention.self.key.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.7.attention.self.key.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.7.attention.self.value.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.7.attention.self.value.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.7.attention.output.dense.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.7.attention.output.dense.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.7.crossattention.self.query.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.7.crossattention.self.query.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.7.crossattention.self.key.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.7.crossattention.self.key.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.7.crossattention.self.value.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.7.crossattention.self.value.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.7.crossattention.output.dense.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.7.crossattention.output.dense.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.7.intermediate.dense.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.7.intermediate.dense.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.7.output.dense.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.7.output.dense.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.8.attention.self.query.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.8.attention.self.query.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.8.attention.self.key.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.8.attention.self.key.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.8.attention.self.value.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.8.attention.self.value.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.8.attention.output.dense.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.8.attention.output.dense.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.8.crossattention.self.query.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.8.crossattention.self.query.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.8.crossattention.self.key.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.8.crossattention.self.key.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.8.crossattention.self.value.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.8.crossattention.self.value.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.8.crossattention.output.dense.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.8.crossattention.output.dense.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.8.intermediate.dense.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.8.intermediate.dense.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.8.output.dense.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.8.output.dense.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.9.attention.self.query.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.9.attention.self.query.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.9.attention.self.key.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.9.attention.self.key.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.9.attention.self.value.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.9.attention.self.value.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.9.attention.output.dense.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.9.attention.output.dense.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.9.crossattention.self.query.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.9.crossattention.self.query.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.9.crossattention.self.key.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.9.crossattention.self.key.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.9.crossattention.self.value.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.9.crossattention.self.value.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.9.crossattention.output.dense.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.9.crossattention.output.dense.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.9.intermediate.dense.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.9.intermediate.dense.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.9.output.dense.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.9.output.dense.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.10.attention.self.query.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.10.attention.self.query.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.10.attention.self.key.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.10.attention.self.key.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.10.attention.self.value.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.10.attention.self.value.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.10.attention.output.dense.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.10.attention.output.dense.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.10.crossattention.self.query.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.10.crossattention.self.query.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.10.crossattention.self.key.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.10.crossattention.self.key.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.10.crossattention.self.value.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.10.crossattention.self.value.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.10.crossattention.output.dense.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.10.crossattention.output.dense.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.10.intermediate.dense.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.10.intermediate.dense.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.10.output.dense.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.10.output.dense.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.11.attention.self.query.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.11.attention.self.query.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.11.attention.self.key.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.11.attention.self.key.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.11.attention.self.value.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.11.attention.self.value.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.11.attention.output.dense.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.11.attention.output.dense.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.11.crossattention.self.query.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.11.crossattention.self.query.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.11.crossattention.self.key.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.11.crossattention.self.key.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.11.crossattention.self.value.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.11.crossattention.self.value.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.11.crossattention.output.dense.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.11.crossattention.output.dense.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.11.intermediate.dense.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.11.intermediate.dense.lora_B.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.11.output.dense.lora_A.weight\n",
      "text_decoder.base_model.model.bert.encoder.layer.11.output.dense.lora_B.weight\n"
     ]
    }
   ],
   "source": [
    "for n, p in model.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        print(n)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
